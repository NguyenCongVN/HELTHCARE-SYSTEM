{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9e1993",
   "metadata": {},
   "source": [
    "# Data Analysis (Spark)\n",
    "Once we have made the data ready for analysis, we have to perform below analysis queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "366e6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# # Get the SparkContext\n",
    "# sc = spark.sparkContext\n",
    "\n",
    "# # Set the logging level to VERBOSE\n",
    "# sc.setLogLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6569759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.bool = np.bool_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01ce39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/02 19:04:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/02 19:05:10 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist\n",
      "24/06/02 19:05:10 WARN HiveConf: HiveConf of name hive.enforce.bucketing does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        namespace|\n",
      "+-----------------+\n",
      "|       adidasshop|\n",
      "|          default|\n",
      "|            eshop|\n",
      "|healthcare_system|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HealthcareDataAnalysis\") \\\n",
    "    .config(\"spark.jars\", \"/home/hduser/Downloads/test/usr/share/java/mysql-connector-j-8.4.0.jar\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Show all the Databases of Hive\n",
    "df = spark.sql(\"show databases\").show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8a56867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------------+----------+-----------------+----------------+------------+----------+\n",
      "|claim_id|patient_id|    disease_name|    sub_id|claim_or_rejected|      claim_type|claim_amount|claim_date|\n",
      "+--------+----------+----------------+----------+-----------------+----------------+------------+----------+\n",
      "|       1|    187158|    Galactosemia|SUBID10000|                N| claims of value|     79874.0|1949-03-14|\n",
      "|       2|    112766|  Bladder cancer|SUBID10001|                N|claims of policy|    151142.0|1970-03-16|\n",
      "|       3|    199252|   Kidney cancer|SUBID10002|                N| claims of value|     59924.0|2008-02-03|\n",
      "|       4|    133424|         Suicide|SUBID10003|                N|  claims of fact|    143120.0|1995-02-08|\n",
      "|       5|    172579|    Food allergy|SUBID10004|                Y| claims of value|    168634.0|1967-05-23|\n",
      "|       6|    171320|        Whiplash|SUBID10005|                N|claims of policy|     64840.0|1991-10-04|\n",
      "|       7|    107794|      Sunbathing| SUBID1006|                N|  claims of fact|     26800.0|1991-03-26|\n",
      "|       8|    130339|Drug consumption|SUBID10007|                N| claims of value|    177186.0|1946-09-05|\n",
      "|       9|    110377|          Dengue|SUBID10008|                N|  claims of fact|    141123.0|1966-06-20|\n",
      "|      10|    149367|    Head banging|SUBID10009|                N| claims of value|     88540.0|1945-12-29|\n",
      "|      11|    156168| Fanconi anaemia| SUBID1010|                N| claims of value|     29150.0|1999-01-25|\n",
      "|      12|    114241|   Breast cancer|SUBID10011|                Y| claims of value|     40897.0|1975-02-08|\n",
      "|      13|    146382|         Anthrax|SUBID10012|                N| claims of value|     75983.0|1985-02-12|\n",
      "|      14|    132748| Cystic fibrosis|SUBID10013|                N|  claims of fact|    192340.0|2014-07-30|\n",
      "|      15|    167340|    Galactosemia|SUBID10014|                N| claims of value|    118628.0|2003-12-18|\n",
      "|      16|    135184|          Dengue|SUBID10015|                Y| claims of value|    100224.0|1986-08-02|\n",
      "|      17|    179662|        Smallpox|SUBID10016|                N| claims of value|     42860.0|1955-01-20|\n",
      "|      18|    184479|  Pollen allergy|SUBID10017|                N|claims of policy|    161786.0|2017-06-01|\n",
      "|      19|    156988|   Breast cancer|SUBID10018|                N|  claims of fact|     66129.0|1956-01-04|\n",
      "|      20|    132870|        Glaucoma|SUBID10019|                N| claims of value|    182552.0|1948-07-26|\n",
      "+--------+----------+----------------+----------+-----------------+----------------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HealthcareDataAnalysis\") \\\n",
    "    .config(\"spark.jars\", \"/home/hduser/Downloads/test/usr/share/java/mysql-connector-j-8.4.0.jar\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.sql(\"SELECT * FROM healthcare_System.claims\").show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a64a5191",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HealthcareDataAnalysis\") \\\n",
    "    .config(\"spark.jars\", \"/home/hduser/Downloads/test/usr/share/java/mysql-connector-j-8.4.0.jar\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de3d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkdf = spark.sql(\"use healthcare_System;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26758fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+-----------+\n",
      "|        namespace|       tableName|isTemporary|\n",
      "+-----------------+----------------+-----------+\n",
      "|healthcare_system|          claims|      false|\n",
      "|healthcare_system|         disease|      false|\n",
      "|healthcare_system|  group_subgroup|      false|\n",
      "|healthcare_system|     groups_tble|      false|\n",
      "|healthcare_system|hospital_details|      false|\n",
      "|healthcare_system| patient_details|      false|\n",
      "|healthcare_system|        subgroup|      false|\n",
      "|healthcare_system|      subscriber|      false|\n",
      "+-----------------+----------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print all the tables which are present in the healthcare_system database.\n",
    "sparkdf = spark.sql(\"show tables\")\n",
    "# Chuyển đổi sang Pandas DataFrame và lưu thành CSV\n",
    "pd_df = sparkdf.toPandas()\n",
    "pd_df.to_csv('Spark Outputs/Database.csv', index=False)\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dd0a51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------+\n",
      "|         col_name|data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|         claim_id|      int|   null|\n",
      "|       patient_id|      int|   null|\n",
      "|     disease_name|   string|   null|\n",
      "|           sub_id|   string|   null|\n",
      "|claim_or_rejected|   string|   null|\n",
      "|       claim_type|   string|   null|\n",
      "|     claim_amount|   double|   null|\n",
      "|       claim_date|     date|   null|\n",
      "+-----------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the claims table\n",
    "sparkdf = spark.sql(\"desc claims\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/Claims_tble.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bfca269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|  disease_id|      int|   null|\n",
      "|disease_name|   string|   null|\n",
      "|   subgrp_id|   string|   null|\n",
      "+------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the disease table\n",
    "sparkdf = spark.sql(\"desc disease\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/disease_tble.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab450a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|         grp_sk|      int|   null|\n",
      "|         grp_id|   string|   null|\n",
      "|       grp_name|   string|   null|\n",
      "|premium_written|      int|   null|\n",
      "|           city|   string|   null|\n",
      "|       zip_code|      int|   null|\n",
      "|        country|   string|   null|\n",
      "|       grp_type|   string|   null|\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the groups table\n",
    "sparkdf = spark.sql(\"desc groups_tble\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/groups_tble.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55c95cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------+\n",
      "| col_name|data_type|comment|\n",
      "+---------+---------+-------+\n",
      "|grpsub_sk|      int|   null|\n",
      "|     g_id|   string|   null|\n",
      "|     s_id|   string|   null|\n",
      "+---------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the grp_subgrp table\n",
    "sparkdf = spark.sql(\"desc group_subgroup\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/grp_subgrp_tble.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82f1cff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|  hospital_id|   string|   null|\n",
      "|hospital_name|   string|   null|\n",
      "|         city|   string|   null|\n",
      "|        state|   string|   null|\n",
      "|      country|   string|   null|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the hospital table\n",
    "sparkdf = spark.sql(\"desc hospital_details\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/hospital_tble.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4eb94bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+-------+\n",
      "|          col_name|data_type|comment|\n",
      "+------------------+---------+-------+\n",
      "|        patient_id|      int|   null|\n",
      "|      patient_name|   string|   null|\n",
      "|    patient_gender|   string|   null|\n",
      "|patient_birth_date|     date|   null|\n",
      "|     patient_phone|   string|   null|\n",
      "|      disease_name|   string|   null|\n",
      "|              city|   string|   null|\n",
      "|       hospital_id|   string|   null|\n",
      "+------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the patient table\n",
    "sparkdf = spark.sql(\"desc patient_details\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/patient_tble.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11071fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|      subgrp_sk|      int|   null|\n",
      "|      subgrp_id|   string|   null|\n",
      "|    subgrp_name|   string|   null|\n",
      "|monthly_premium|   double|   null|\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the subgroup table\n",
    "sparkdf = spark.sql(\"desc subgroup\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/subgroup_tble.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "487012d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|    sub_id|   string|   null|\n",
      "|first_name|   string|   null|\n",
      "| last_name|   string|   null|\n",
      "|    street|   string|   null|\n",
      "|birth_date|     date|   null|\n",
      "|    gender|   string|   null|\n",
      "|     phone|   string|   null|\n",
      "|      city|   string|   null|\n",
      "|  zip_code|      int|   null|\n",
      "|   country|   string|   null|\n",
      "| subgrp_id|   string|   null|\n",
      "|  elig_ind|   string|   null|\n",
      "|  eff_date|     date|   null|\n",
      "| term_date|     date|   null|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the subscriber table\n",
    "sparkdf = spark.sql(\"desc subscriber\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/subscriber_tble.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bba437b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which disease having maximum number of claims.\n",
    "results = spark.sql(\"select disease_name,count(claim_id) as \\\n",
    "                    max from claims group by disease_name order by max desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d92f2816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+\n",
      "|   disease_name|max|\n",
      "+---------------+---+\n",
      "|   Head banging|  3|\n",
      "|    Pet allergy|  3|\n",
      "|   Galactosemia|  3|\n",
      "|       Glaucoma|  3|\n",
      "|        Anthrax|  3|\n",
      "|Phenylketonuria|  3|\n",
      "|            Flu|  2|\n",
      "| Bladder cancer|  2|\n",
      "|       Diabetes|  2|\n",
      "|Fanconi anaemia|  2|\n",
      "|     Lymphedema|  2|\n",
      "|        Cholera|  2|\n",
      "|         Stroke|  2|\n",
      "|         Scurvy|  2|\n",
      "|  Rett Syndrome|  2|\n",
      "|        Measles|  2|\n",
      "|   Mold allergy|  2|\n",
      "|        Malaria|  2|\n",
      "|        Choking|  2|\n",
      "|         Asthma|  2|\n",
      "+---------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.toPandas().to_csv('Spark Outputs/query1.csv')\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3754f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find those Subscribers having age less than 30 and they subscribe any subgroup\n",
    "subscriber_ages = spark.sql(\"select birth_date,current_date() as CurrentDate,\\\n",
    "          year(current_date())-year(birth_date) as age from subscriber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d61882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = subscriber_ages.filter(subscriber_ages.age < 30).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "563e43b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscribers having age less than 30 -->  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Subscribers having age less than 30 --> \",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3f2f00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  g_id|max|\n",
      "+------+---+\n",
      "|GRP143|  2|\n",
      "|GRP147|  2|\n",
      "|GRP104|  2|\n",
      "|GRP105|  1|\n",
      "|GRP108|  1|\n",
      "|GRP131|  1|\n",
      "|GRP102|  1|\n",
      "|GRP103|  1|\n",
      "|GRP112|  1|\n",
      "|GRP101|  1|\n",
      "|GRP123|  1|\n",
      "|GRP114|  1|\n",
      "|GRP127|  1|\n",
      "|GRP133|  1|\n",
      "|GRP122|  1|\n",
      "|GRP138|  1|\n",
      "|GRP148|  1|\n",
      "|GRP142|  1|\n",
      "|GRP157|  1|\n",
      "|GRP126|  1|\n",
      "+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out which group has maximum subgroups.\n",
    "sparkdf = spark.sql(\"select g_id,count(s_id) as max from group_subgroup group by g_id order by max desc\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/query3.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da47fe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|       hospital_name|total_patient|\n",
      "+--------------------+-------------+\n",
      "|   Manipal Hospitals|            9|\n",
      "|Apollo Hospitals ...|            8|\n",
      "|Medanta The Medicity|            7|\n",
      "|Jaslok Hospital a...|            6|\n",
      "|Indraprastha Apol...|            5|\n",
      "|Fortis Hospital M...|            4|\n",
      "|PGIMER - Postgrad...|            4|\n",
      "|Apollo Hospital -...|            4|\n",
      "|Bombay Hospital &...|            3|\n",
      "|Yashoda Hospital ...|            3|\n",
      "|Apollo Health Cit...|            3|\n",
      "|King Edward Memor...|            3|\n",
      "|Lilavati Hospital...|            2|\n",
      "|The Christian Med...|            2|\n",
      "|Fortis Hiranandan...|            2|\n",
      "|Fortis Flt. Lt. R...|            1|\n",
      "|P. D. Hinduja Nat...|            1|\n",
      "|Sir Ganga Ram Hos...|            1|\n",
      "|All India Institu...|            1|\n",
      "|Breach Candy Hosp...|            1|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out hospital which serve most number of patients\n",
    "sparkdf = spark.sql(\"select hospital_name,count(patient_id) as total_patient \\\n",
    "          from hospital_details join patient_details on hospital_details.hospital_id=patient_details.hospital_id \\\n",
    "          group by hospital_name order by total_patient desc\")\n",
    "\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/query4.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "598f14f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+\n",
      "|        subgrp_name|cunt|\n",
      "+-------------------+----+\n",
      "|            Therapy|  14|\n",
      "|              Viral|  11|\n",
      "|Deficiency Diseases|  11|\n",
      "|         Hereditary|  11|\n",
      "|          Allergies|  10|\n",
      "|         Physiology|  10|\n",
      "|           Accident|  10|\n",
      "|             Cancer|   9|\n",
      "|     Self inflicted|   7|\n",
      "| Infectious disease|   7|\n",
      "+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out which subgroups subscribe most number of times\n",
    "sparkdf = spark.sql(\"select subgrp_name, \\\n",
    "          count(sub_id) as cunt from subgroup join \\\n",
    "          subscriber on subgroup.subgrp_id = subscriber.subgrp_id  group by subgrp_name order by cunt desc\")\n",
    "\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/query5.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38ca900a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+\n",
      "|claim_or_rejected|count(claim_id)|\n",
      "+-----------------+---------------+\n",
      "|                Y|             18|\n",
      "|                N|             52|\n",
      "+-----------------+---------------+\n",
      "\n",
      "The above Result shows the total 52 claims were rejected\n"
     ]
    }
   ],
   "source": [
    "# Find out total number of claims which were rejected\n",
    "sparkdf = spark.sql(\"select claim_or_rejected,count(claim_id) from claims group by claim_or_rejected\")\n",
    "\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/query6.csv')\n",
    "sparkdf.show()\n",
    "print(\"The above Result shows the total 52 claims were rejected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7511581f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|        city|maxclaim|\n",
      "+------------+--------+\n",
      "|      Mysore|       2|\n",
      "|    Amravati|       2|\n",
      "|   Kamarhati|       2|\n",
      "|    Jabalpur|       2|\n",
      "|Bihar Sharif|       2|\n",
      "|   Ghaziabad|       2|\n",
      "|       Morbi|       2|\n",
      "|  Karimnagar|       2|\n",
      "|   Bangalore|       1|\n",
      "|     Udaipur|       1|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From where most claims are comming (city)\n",
    "sparkdf = spark.sql(\"select patient_details.city,count(claim_id) as maxclaim from patient_details join claims on \\\n",
    "          claims.patient_id = patient_details.patient_id group by patient_details.city order by maxclaim desc limit 10\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/query7.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c816111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|grp_type|count(grp_id)|\n",
      "+--------+-------------+\n",
      "|   Govt.|            7|\n",
      "| Private|           51|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which groups of policies subscriber subscribe mostly Goverment or private\n",
    "sparkdf = spark.sql(\"select grp_type,count(grp_id) from groups_tble group by grp_type\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/query8.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96460808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hduser/miniconda3/envs/bigdata/lib/python3.9/site-packages/pyspark/sql/dataframe.py:138: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|grp_type|count(g_id)|\n",
      "+--------+-----------+\n",
      "|   Govt.|         35|\n",
      "| Private|        347|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = spark.sql(\"select group_subgroup.g_id from subscriber \\\n",
    "          join group_subgroup on group_subgroup.s_id = subscriber.subgrp_id\")\n",
    "\n",
    "res.registerTempTable(\"grp_tble\")\n",
    "\n",
    "sparkdf = spark.sql(\"select grp_type,count(grp_tble.g_id) from groups_tble \\\n",
    "          join grp_tble on groups_tble.grp_id = grp_tble.g_id group by grp_type\")\n",
    "\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/query9.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28c891c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Above Result shows that most of the subscribers subscribe private groups\n"
     ]
    }
   ],
   "source": [
    "print(\"The Above Result shows that most of the subscribers subscribe private groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e9dca22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|subgrp_id|avg(monthly_premium)|\n",
      "+---------+--------------------+\n",
      "|     S101|              3000.0|\n",
      "|     S102|              1000.0|\n",
      "|     S103|              2000.0|\n",
      "|     S104|              1500.0|\n",
      "|     S105|              2300.0|\n",
      "|     S106|              1200.0|\n",
      "|     S107|              3200.0|\n",
      "|     S108|              1500.0|\n",
      "|     S109|              2000.0|\n",
      "|     S110|              1000.0|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average monthly premium subscriber pay to insurance company subgroup.\n",
    "sparkdf = spark.sql(\"select subgroup.subgrp_id,avg(monthly_premium) from subscriber right join \\\n",
    "          subgroup on subscriber.subgrp_id = subgroup.subgrp_id group by \\\n",
    "          subgroup.subgrp_id order by subgroup.subgrp_id\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/query10.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7771fc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|grp_id|premium_written|\n",
      "+------+---------------+\n",
      "|GRP147|          99000|\n",
      "|GRP131|          99000|\n",
      "|GRP123|          99000|\n",
      "|GRP118|          97000|\n",
      "|GRP154|          95000|\n",
      "|GRP133|          93000|\n",
      "|GRP157|          92000|\n",
      "|GRP134|          90000|\n",
      "|GRP143|          90000|\n",
      "|GRP106|          89000|\n",
      "|GRP129|          88000|\n",
      "|GRP152|          87000|\n",
      "|GRP153|          86000|\n",
      "|GRP150|          84000|\n",
      "|GRP124|          81000|\n",
      "|GRP122|          79000|\n",
      "|GRP115|          79000|\n",
      "|GRP121|          78000|\n",
      "|GRP109|          78000|\n",
      "|GRP101|          72000|\n",
      "+------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out Which group is most profitable\n",
    "sparkdf = spark.sql(\"select grp_id,premium_written from groups_tble order by premium_written desc\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/query11.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e20b366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---+\n",
      "|patient_id|patient_name|age|\n",
      "+----------+------------+---+\n",
      "|    194166|          NA| 11|\n",
      "|    197441|    Deependu| 20|\n",
      "+----------+------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all the patients below age of 18 who admit for cancer\n",
    "\n",
    "sparkdf = spark.sql(\"select patient_id, patient_name,\\\n",
    "          year(current_date())-year(patient_birth_date) as age from patient_details \\\n",
    "          where disease_name like '%cancer' order by age limit 2\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/query12.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fee0245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+------------------+\n",
      "|patient_name|patient_gender|patient_birth_date|\n",
      "+------------+--------------+------------------+\n",
      "|   Anjushree|          Male|        1982-06-28|\n",
      "|  Chitranjan|        Female|        2020-10-27|\n",
      "|      Gensho|          Male|        1991-07-27|\n",
      "|  Vaijayanti|          Male|        1969-04-06|\n",
      "|       Aakar|        Female|        1990-04-24|\n",
      "|          NA|        Female|        1959-01-06|\n",
      "|       Saroj|        Female|        1953-07-21|\n",
      "|     Bhagvan|        Female|        2011-02-26|\n",
      "|  Dharmadaas|          Male|        1964-10-25|\n",
      "|       Umang|        Female|        2017-02-26|\n",
      "|          NA|          Male|        1955-06-03|\n",
      "|      Kishan|          Male|        1955-06-30|\n",
      "|          NA|        Female|        1953-04-04|\n",
      "|     Devnath|        Female|        1982-02-22|\n",
      "|      Harbir|        Female|        1960-02-24|\n",
      "|       Ekant|          Male|        1969-11-01|\n",
      "|          NA|          Male|        2013-10-30|\n",
      "|          NA|          Male|        1956-04-04|\n",
      "|       Lalit|        Female|        1978-04-30|\n",
      "|     Ujjawal|          Male|        1965-12-31|\n",
      "+------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List patients who have cashless insura0nce and have total charges greater than or equal for Rs. 50,000.\n",
    "\n",
    "sparkdf = spark.sql(\"select patient_name,patient_gender,patient_birth_date \\\n",
    "          from patient_details join claims on patient_details.patient_id = claims.patient_id \\\n",
    "          where claim_amount >= 50000 and claim_type = 'claims of value'\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/query13.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7eeaeb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|patient_name|\n",
      "+------------+\n",
      "|     Upasana|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List female patients over the age of 40 that have undergone knee surgery in the past year\n",
    "\n",
    "sparkdf = spark.sql(\"select patient_name from patient_details where patient_gender = 'Female' and disease_name = 'Heart Attack'\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs/query14.csv')\n",
    "sparkdf.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
